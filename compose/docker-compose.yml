version: "3.8"
services:
  mlflow-db:
    image: postgres:14
    environment:
      POSTGRES_USER: mlflow
      POSTGRES_PASSWORD: mlflow
      POSTGRES_DB: mlflow
    volumes: [mlflow_db:/var/lib/postgresql/data]
    networks: [mlops]

  minio:
    image: minio/minio:latest
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER:-minio}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD:-minio123}
    ports: ["9000:9000", "9001:9001"]
    volumes: [minio_data:/data]
    networks: [mlops]

  minio-mc:
    image: minio/mc:latest
    depends_on: [minio]
    entrypoint: |
      sh -c "
      mc alias set local http://minio:9000 ${MINIO_ROOT_USER:-minio} ${MINIO_ROOT_PASSWORD:-minio123} &&
      mc mb -p local/mlflow || true &&
      mc anonymous set download local/mlflow || true &&
      tail -f /dev/null
      "
    networks: [mlops]

  mlflow:
    image: bitnami/mlflow:2
    depends_on: [mlflow-db, minio, minio-mc]
    environment:
      MLFLOW_BACKEND_STORE_URI: ${MLFLOW_BACKEND_DB}
      MLFLOW_DEFAULT_ARTIFACT_ROOT: ${MLFLOW_ARTIFACT_S3}
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
      MLFLOW_S3_ENDPOINT_URL: ${MLFLOW_S3_ENDPOINT_URL}
    command: mlflow server --host 0.0.0.0 --port 5001 --backend-store-uri ${MLFLOW_BACKEND_DB} --default-artifact-root ${MLFLOW_ARTIFACT_S3}
    ports: ["${MLFLOW_PORT}:5001"]
    networks: [mlops]

  imdb-app:
    build:
      context: ..
      dockerfile: docker/Dockerfile.v1
    depends_on: [mlflow]
    environment:
      MODEL_NAME: sentiment-imdb
      MODEL_STAGE: Staging
      # Доступ к MLflow/MinIO внутри сети compose
      MLFLOW_TRACKING_URI: http://mlflow:5001
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
      MLFLOW_S3_ENDPOINT_URL: ${MLFLOW_S3_ENDPOINT_URL}
    ports: ["8088:8080"]
    networks: [mlops]

  prometheus:
    image: prom/prometheus:latest
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml:ro
    ports: ["${PROMETHEUS_PORT}:9090"]
    networks: [mlops]

  grafana:
    image: grafana/grafana:latest
    environment:
      GF_SECURITY_ADMIN_USER: admin
      GF_SECURITY_ADMIN_PASSWORD: admin
      GF_PATHS_PROVISIONING: /etc/grafana/provisioning
    volumes:
      - ./grafana-datasource.yml:/etc/grafana/provisioning/datasources/datasource.yml:ro
    ports: ["${GRAFANA_PORT}:3000"]
    networks: [mlops]

  airflow-db:
    image: postgres:14
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes: [airflow_db:/var/lib/postgresql/data]
    networks: [mlops]

  airflow-scheduler:
    image: apache/airflow:2.8.3
    depends_on: [airflow-db]
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-db:5432/airflow
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW_FERNET_KEY}
      AIRFLOW__WEBSERVER__RBAC: "True"
      PYTHONPATH: /opt/airflow
      MLFLOW_TRACKING_URI: http://mlflow:5001
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
      MLFLOW_S3_ENDPOINT_URL: ${MLFLOW_S3_ENDPOINT_URL}
    volumes:
      - ../airflow/dags:/opt/airflow/dags
      - ../training:/opt/airflow/training
    command: bash -c "airflow db init && airflow scheduler"
    networks: [mlops]

  airflow-webserver:
    image: apache/airflow:2.8.3
    depends_on: [airflow-db, airflow-scheduler]
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-db:5432/airflow
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW_FERNET_KEY}
      PYTHONPATH: /opt/airflow
    volumes:
      - ../airflow/dags:/opt/airflow/dags
      - ../training:/opt/airflow/training
    ports: ["8080:8080"]
    command: bash -c "airflow webserver"
    networks: [mlops]

networks: { mlops: {} }
volumes: { mlflow_db: {}, minio_data: {}, airflow_db: {} }